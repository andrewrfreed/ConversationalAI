{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Your Own Classifier - Code Examples\n",
    "\n",
    "This Jupyter notebook builds the classifiers demonstrated in the 'Building Your Own Classifier' chapter of 'Creating Virtual Assistants'.\n",
    "Run this notebook on the data sets provided in the book's supporting material first, then feel free to experiment with your own data or your own classification algorithms!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pre-requisites used by this notebook\n",
    "# Every notebook starts by installing the modules it needs!\n",
    "!pip install scikit-learn==0.23.2\n",
    "!pip install eli5==0.10.1\n",
    "!pip install pandas==1.0.5\n",
    "!pip install scipy==1.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After installing the libraries you need to import them into the notebook also.\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import eli5\n",
    "import scipy.sparse\n",
    "import math\n",
    "from IPython.display import display\n",
    "\n",
    "# The eli5 library causes a warning from scikit-learn when you import it.\n",
    "# It is safe to ignore while exploring this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell disables vertical scrollbars in the notebook - useful because we have so much output.\n",
    "\n",
    "This is completely optional and does not work on JupyterLab.  You can alternately:\n",
    "* Remove the cell and let the notebook autoscroll\n",
    "* Remove the cell and use the menu option `Cell->All Outputs->Toggle Scrolling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    //If this cell fails, you can safely remove it.\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions we will reuse throughout all examples\n",
    "\n",
    "'''\n",
    "Reads a CSV file into a pandas dataframe.\n",
    "The CSV file is assumed to not have headers\n",
    "'''\n",
    "def read_data_from_csv_file(path:str):\n",
    "    data = pd.read_csv(filepath_or_buffer=path, names=[\"utterance\", \"label\"])\n",
    "    return data\n",
    "\n",
    "'''\n",
    "Given a dataframe with multiple possible target classes, we will rewrite all\n",
    "other classes besides the specified class to \"negative example\".\n",
    "If the input dataframe has classes \"reset_password\", \"store_hours\", and \"store_location\"\n",
    "then the output dataframe will have classes \"reset_password\" and \"negative_example\" when the\n",
    "target class is \"reset_password\"\n",
    "'''\n",
    "def convert_data_for_target_class(target_class: str, all_data: pd.DataFrame):\n",
    "    class_data = all_data.copy()\n",
    "    class_data[\"label\"] = class_data[\"label\"].apply(\n",
    "        lambda x: x if x == target_class else \"negative_example\"\n",
    "    )\n",
    "    return class_data\n",
    "\n",
    "'''\n",
    "Trains a binary classifier to detect a single target_class from a set of training data\n",
    "provided as the 'data' input.\n",
    "'''\n",
    "def train_binary_classifier_pipeline(target_class: str, data: pd.DataFrame):\n",
    "    class_data = convert_data_for_target_class(target_class, data)\n",
    "\n",
    "    X = class_data[\"utterance\"]\n",
    "    Y = class_data[\"label\"]\n",
    "\n",
    "    # CountVectorizer turns a text phrase into an array of numbers by counting how many times each word shows up.\n",
    "    # It is common to remove common words, also known as \"stop words\", like \"the\", since these common words\n",
    "    # are assumed to not have significant signal in determining the classification of a text phrase\n",
    "    vec = CountVectorizer(stop_words=[\"the\"])\n",
    "    clf = LogisticRegressionCV(max_iter=500, cv=3)\n",
    "    binary_classifier_pipeline = make_pipeline(vec, clf)\n",
    "    binary_classifier_pipeline.fit(X, Y)\n",
    "\n",
    "    return vec, clf, binary_classifier_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the simplest binary classifier - just two classes and we only train one classifier (for reset_password)\n",
    "\n",
    "training_data = read_data_from_csv_file(\"training_2class_small.csv\")\n",
    "\n",
    "print(\"Training data for this example\")\n",
    "display(training_data)\n",
    "\n",
    "# First we demonstrate a single binary classifier before demonstrating several classifiers.\n",
    "# Since each classifier has the same training data and pipeline construction, they will each have the same features.\n",
    "vec, clf, pipe = train_binary_classifier_pipeline(\"reset_password\", training_data)\n",
    "print()\n",
    "print(\"List of features used in the bag of words classifier:\")\n",
    "print(\n",
    "    [\n",
    "        list((i, vec.get_feature_names()[i]))\n",
    "        for i in range(len(vec.get_feature_names()))\n",
    "    ])\n",
    "print()\n",
    "\n",
    "# Exploring the features produced for a single utterance\n",
    "# We use a sparse matrix here for ease of visualization.\n",
    "# Note that sparse matrices are more memory intensive and most algorithms internally use dense matrices.\n",
    "# Thus we need a special conversion from the vectorization transform to a sparse matrix as shown below.\n",
    "\n",
    "reset_my_password_sparse_features = pd.DataFrame.sparse.from_spmatrix(\n",
    "    vec.transform([\"reset my password\"])\n",
    ")\n",
    "\n",
    "how_late_are_you_open_sparse_features = pd.DataFrame.sparse.from_spmatrix(\n",
    "    vec.transform([\"how late are you open\"])\n",
    ")\n",
    "\n",
    "combined_df = pd.concat(\n",
    "    [reset_my_password_sparse_features, how_late_are_you_open_sparse_features]\n",
    ")\n",
    "\n",
    "combined_df[\"Output\"] = [1, 0]\n",
    "combined_df.insert(\n",
    "    0, \"Utterance\", [\"reset my password\", \"how late are you open\"]\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"Features extracted from multiple inputs:\")\n",
    "display(combined_df.reset_index(drop=True))\n",
    "\n",
    "print(\"Feature weights\")\n",
    "display(eli5.show_weights(clf, vec=vec, top=20, target_names=pipe.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing probabilities\n",
    "print(\"Exploring the phrase 'reset my password' with a binary classifier trained on two examples\")\n",
    "# We add 0.189 for the bias parameter, and each word of \"reset\", \"my\", \"password\" has equal weight (0.378)\n",
    "reset_password_sigmoid_score = 0.189+0.378+0.378+0.378\n",
    "\n",
    "logit=math.exp(reset_password_sigmoid_score)\n",
    "#First method of computing probability\n",
    "prob=1/(1+math.exp(-1*reset_password_sigmoid_score))\n",
    "#Second method of computing probability\n",
    "prob = math.exp(reset_password_sigmoid_score) / (1+math.exp(reset_password_sigmoid_score))\n",
    "\n",
    "print(f\"sigmoid score: {reset_password_sigmoid_score}\")\n",
    "print(f\"logit: {logit}\")\n",
    "print(f\"probability: {prob}\")\n",
    "\n",
    "# The probabilities above should match the prediction generated from the classifier\n",
    "print()\n",
    "print(\"Predicting the phrase 'reset my password' with binary classifier\")\n",
    "display(eli5.show_prediction(clf, \"reset my password\", vec=vec, show_feature_values=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's train and test a series of binary classifiers.  \n",
    "# Here we will train three classifiers: reset_password, store_hours, store_location\n",
    "# We will examine the weights learned by each classifier and how they predict the phrase \"reset my password\"\n",
    "print(\"Training three binary classifiers, each with one (repeated) example\")\n",
    "training_data = read_data_from_csv_file(\"training_3class_small.csv\")\n",
    "\n",
    "print(\"Training data for this example - just one example for each class (repeated multiple times)\")\n",
    "display(training_data)\n",
    "\n",
    "print(\"Training three classifiers\")\n",
    "reset_password_vec, reset_password_clf, reset_password_pipe = \\\n",
    "    train_binary_classifier_pipeline(\"reset_password\", training_data)\n",
    "store_hours_vec, store_hours_clf, store_hours_pipe = \\\n",
    "    train_binary_classifier_pipeline(\"store_hours\", training_data)\n",
    "store_location_vec, store_location_clf, store_location_pipe = \\\n",
    "    train_binary_classifier_pipeline(\"store_location\", training_data)\n",
    "\n",
    "print(\"Examining the feature weights learned by each classifier\")\n",
    "display(eli5.show_weights(reset_password_clf, vec=reset_password_vec, top=20, target_names=reset_password_pipe.classes_))\n",
    "display(eli5.show_weights(store_hours_clf, vec=store_hours_vec, top=20, target_names=store_hours_pipe.classes_))\n",
    "display(eli5.show_weights(store_location_clf, vec=store_location_vec, top=20, target_names=store_location_pipe.classes_))\n",
    "\n",
    "\n",
    "print(\"Predicting the phrase 'reset my password' with reset_password classifier\")\n",
    "display(eli5.show_prediction(reset_password_clf, \"reset my password\", vec=reset_password_vec, show_feature_values=True))\n",
    "\n",
    "print(\"Predicting the phrase 'reset my password' with store_hours classifier\")\n",
    "display(eli5.show_prediction(store_hours_clf, \"reset my password\", vec=store_hours_vec, show_feature_values=True))\n",
    "\n",
    "print(\"Predicting the phrase 'reset my password' with store_location classifier\")\n",
    "display(eli5.show_prediction(store_location_clf, \"reset my password\", vec=store_location_vec, show_feature_values=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All-in-one classifier\n",
    "print(\"Training an all-in-one classifier on a small data set\")\n",
    "\n",
    "#Smallest 3 class\n",
    "training_data = read_data_from_csv_file(\"training_3class_small.csv\")\n",
    "\n",
    "print(\"Training data for this example\")\n",
    "display(training_data)\n",
    "\n",
    "X = training_data[\"utterance\"]\n",
    "Y = training_data[\"label\"]\n",
    "\n",
    "# CountVectorizer turns a text phrase into an array of numbers by counting how many times each word shows up.\n",
    "# It is common to remove common words, also known as \"stop words\", like \"the\", since these common words\n",
    "# are assumed to not have significant signal in determining the classification of a text phrase\n",
    "vec = CountVectorizer(stop_words=[\"the\"])\n",
    "clf = LogisticRegressionCV(max_iter=500, cv=3)  \n",
    "pipe = make_pipeline(vec, clf)\n",
    "pipe.fit(X, Y)\n",
    "print(\"An all-in-one classifier was trained.\")\n",
    "\n",
    "\n",
    "#WARNING - these weights are for an all-in-one classifier, NOT the binary classifiers!!\n",
    "print(\"The all-in-one classifier learned these weights from the training data\")\n",
    "display(eli5.show_weights(clf, vec=vec, top=10, target_names=pipe.classes_))\n",
    "\n",
    "# # If you are NOT in Jupyter notebook, you can print this way instead\n",
    "# explanation = eli5.explain_weights(clf, vec=vec, top=10, target_names=pipe.classes_)\n",
    "# print(eli5.formatters.text.format_as_text(explanation))\n",
    "\n",
    "print()\n",
    "print(\"Predictions from an all-in-one classifier.\")\n",
    "print(\"Since the classifier is all-in-one, the probabilities will add up to 1.\")\n",
    "print(\"Predictions from 'reset my password'\")\n",
    "display(eli5.show_prediction(clf, \"reset my password\", vec=vec, show_feature_values=True))\n",
    "print()\n",
    "print(\"Predictions from 'tell me a joke'\")\n",
    "display(eli5.show_prediction(clf, \"tell me a joke\", vec=vec, show_feature_values=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's update the three binary classifiers with more training data\n",
    "# Instead of one single example for each class (repeated several times),\n",
    "# we will train with a more varied set of training data.\n",
    "# Again, we examine the weights learned by each classifier and how they predict the phrase \"reset my password\"\n",
    "print(\"Training three binary classifiers, each with several examples\")\n",
    "training_data = read_data_from_csv_file(\"training_3class_medium.csv\")\n",
    "\n",
    "print(\"Training data for this example\")\n",
    "display(training_data)\n",
    "\n",
    "print(\"Training three classifiers\")\n",
    "reset_password_vec, reset_password_clf, reset_password_pipe = \\\n",
    "    train_binary_classifier_pipeline(\"reset_password\", training_data)\n",
    "store_hours_vec, store_hours_clf, store_hours_pipe = \\\n",
    "    train_binary_classifier_pipeline(\"store_hours\", training_data)\n",
    "store_location_vec, store_location_clf, store_location_pipe = \\\n",
    "    train_binary_classifier_pipeline(\"store_location\", training_data)\n",
    "\n",
    "print(\"Examining the feature weights learned by each classifier\")\n",
    "display(eli5.show_weights(reset_password_clf, vec=reset_password_vec, top=20, target_names=reset_password_pipe.classes_))\n",
    "display(eli5.show_weights(store_hours_clf, vec=store_hours_vec, top=20, target_names=store_hours_pipe.classes_))\n",
    "display(eli5.show_weights(store_location_clf, vec=store_location_vec, top=20, target_names=store_location_pipe.classes_))\n",
    "\n",
    "\n",
    "print(\"Predicting the phrase 'reset my password' with reset_password classifier\")\n",
    "display(eli5.show_prediction(reset_password_clf, \"reset my password\", vec=reset_password_vec, show_feature_values=True))\n",
    "\n",
    "print(\"Predicting the phrase 'reset my password' with store_hours classifier\")\n",
    "display(eli5.show_prediction(store_hours_clf, \"reset my password\", vec=store_hours_vec, show_feature_values=True))\n",
    "\n",
    "print(\"Predicting the phrase 'reset my password' with store_location classifier\")\n",
    "display(eli5.show_prediction(store_location_clf, \"reset my password\", vec=store_location_vec, show_feature_values=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is an alternate exploration of the three classifiers on varied input phrases\n",
    "# Try changing the input file, the test utterances, or the output types\n",
    "training_data = read_data_from_csv_file(\"training_3class_small.csv\")\n",
    "\n",
    "print(\"Training data for this example\")\n",
    "display(training_data)\n",
    "\n",
    "test_utterances_from_training = [\n",
    "    \"reset my password\",\n",
    "    \"Where are you located?\",\n",
    "    \"How late are you open\",\n",
    "]\n",
    "\n",
    "# Try adding new phrases to this list and see what happens!\n",
    "test_unseen_utterances = [\n",
    "    \"I can't login\",\n",
    "    \"I can't remember my password\",\n",
    "    \"When do you close\",\n",
    "    \"tell me a joke\"\n",
    "]\n",
    "\n",
    "'''\n",
    "Runs a single text utterance through an array of text classifiers\n",
    "Prints out the predictions and associated probabilities to those predictions.\n",
    "'''\n",
    "def multi_predict_and_score(one_utterance: str, classifier_pipelines: dict):\n",
    "    print(f\"Utterance '{one_utterance}' predictions with multiple classifiers\")\n",
    "    for target_class in classifier_pipelines:\n",
    "        classifier_pipeline = classifier_pipelines[target_class]\n",
    "        prediction = classifier_pipeline.predict([one_utterance])\n",
    "        probabilities = pd.DataFrame(\n",
    "            classifier_pipeline.predict_proba([one_utterance]),\n",
    "            columns=classifier_pipeline.classes_,\n",
    "        )\n",
    "\n",
    "        # This makes the print-outs look nicer\n",
    "        probabilities.columns = probabilities.columns.str.pad(\n",
    "            width=14, side=\"left\", fillchar=\" \"\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            probabilities.to_string(index=False), \"\\n\",\n",
    "        )\n",
    "'''\n",
    "Builds and tests multiple classifiers\n",
    "'''\n",
    "target_classes = sorted(training_data[\"label\"].unique())\n",
    "classifier_pipelines = {}\n",
    "for target_class in target_classes:\n",
    "    print(\n",
    "        \"**********************************************************************\",\n",
    "        f\"\\nDemonstrating a binary classifier for class {target_class}\",\n",
    "    )\n",
    "\n",
    "    vec, clf, pipe = train_binary_classifier_pipeline(target_class, training_data)\n",
    "    print(\"Predicting the phrase 'reset my password'\")\n",
    "    display(eli5.show_prediction(clf, \"reset my password\", vec=vec, show_feature_values=True))\n",
    "\n",
    "    classifier_pipelines[target_class] = pipe\n",
    "    \n",
    "print(\n",
    "    f\"\\nEvaluating examples from the original training data against '{target_class}'\"\n",
    "    + \"\\n(Note that scikit-learn does NOT perfectly remember training data.)\",\n",
    "    \"\\n\",\n",
    "    \"------------------------------------------------------------------------\",\n",
    ")\n",
    "\n",
    "for utterance in test_utterances_from_training:\n",
    "    multi_predict_and_score(utterance, classifier_pipelines)\n",
    "\n",
    "print(\n",
    "    f\"\\nEvaluating brand new examples not found in the original training data against '{target_class}'.\",\n",
    "    \"\\n\",\n",
    "    \"------------------------------------------------------------------------\",\n",
    ")\n",
    "\n",
    "for utterance in test_unseen_utterances:\n",
    "    multi_predict_and_score(utterance, classifier_pipelines)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
